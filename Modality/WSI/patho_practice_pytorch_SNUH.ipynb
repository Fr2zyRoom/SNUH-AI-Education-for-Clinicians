{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SNUH-AIeducation/SNUH-AI-Education-for-Clinicians/blob/master/Modality/WSI/patho_practice_pytorch_SNUH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segmentation task in digital pathology**\n",
    "디지털 병리 영상을 이용한 딥러닝 기반 segmentation 모델 구축을 진행합니다.  \n",
    "실습에 앞서 필요한 데이터와 라이브러리를 내려받습니다.  \n",
    "[런타임]-[런타임 유형 변경]에서 하드웨어 가속기를 GPU로 변경합니다.  \n",
    "라이브러리를 모두 받은 후에 꼭 **[런타임]-[런타임 다시 시작]**을 눌러주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MMZMfZc1MBW3jjiRlS_Kc6XNgc7VNmeX' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1MMZMfZc1MBW3jjiRlS_Kc6XNgc7VNmeX\" -O wsi.zip && rm -rf ~/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip wsi.zip -d ./wsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install python3-openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall albumentations==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imagecodecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening\n",
    "\n",
    "- Step 1. HuBMAP - Hacking the kidney\n",
    "    - Step 1-1. Dataset specifications\n",
    "    - Step 1-2. Dataset structure\n",
    "        - Step 1-2-1. Whole Slide Image(.tiff)\n",
    "        - Step 1-2-2. Anatomical structure(.json)\n",
    "        - Step 1-2-3. Glomeruli label(.json, RLE)\n",
    "- Step 2. 데이터 전처리: WSI patch generation\n",
    "- Step 3. 데이터셋 구축\n",
    "- Step 4. 모델 학습\n",
    "- Step 5. 평가\n",
    "    - Step 5-1. Patch-wise validation\n",
    "    - Step 5-2. Whole image-wise validation\n",
    "    - Step 5-3. IoU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1. HuBMAP - Hacking the kidney**\n",
    "본 실습에서는 kaggle의 'HuBMAP Glomeruli FTU Segmentation Dataset'을 활용해  \n",
    "디지털 병리 영상에서 `Glomeruli`를 분할하는 segmentation model 구축을 진행합니다.  \n",
    "https://www.kaggle.com/c/hubmap-kidney-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hubmap_results](https://www.kaggleusercontent.com/kf/48229620/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..0EfFqtR2KRpJ4P7fgHoVIg.Z78v2tme4qFKDCFJlrIjw4IUra9v5OoE0AyRZUQ5Sl6DGoCqpMSWDLrU3bxR1zaaS7a_0fRB58gQ7XE75Vk9vlTUO8qaVk8Exy3JW8th1wQC0hlgp1PyRhVfsbWobUv2-9qyTK1Ww5o287PAPh5HROW6xMpvBSyE6WxJUiX9lxWdoRjO9ttJBoMeMio7k_DH0N3Vwr1dCdTt1qpKIns0EACWj7Kx6qWoS9F4SuQlKoj4lQMFnI8p-uGE3S5pDCApbGpcUrRdLvnrL9N1XQbB2gXfnmndMtrFwt6-LDGWdl42bJlzq9Hur5RvDUXvMmWhq8SAb492xX-y-wIm51BBKv8v0lUvwfspt6K64BWggr-c60a8RIBE7ztQVnyDqgJEoPMd8doMf0U8Apjj_ifWSA8-LQvQklrYCLJHGrRxCN4DoEKhcqZ2RZ3tA9r86hQQr0wTLpb9XIyb8w6CJsmd11QF4XUdCl6N_rpdfZcO9xAmUwemB40iTNG_jbMH8GxDTwO0IGlPIfbo3Nv5ay4tfKtIAhdGqt9VkHoG-2vmnX23aL3Of_xLFgHU48HHFnWF7PpbM-qA201oab0UbOPn5_n0tfB1AAV0EoDwLY9w9wqyByTOpomTp_bjkddyKN8_7-CXSpFNs6TwWaG6Aow5p2TJ9kYWx2ZuInscM1Q2NOU.PJl_tL1VktTAUjTWPQUY-Q/__results___files/__results___24_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.kaggle.com/ihelon/hubmap-exploratory-data-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1-1. Dataset specifications\n",
    "Human BioMolecular Atlas Program(HuBMAP)이 NIH의 후원을 통해 개최한 위 competition은  \n",
    "병리 영상에서 `functional tissue units (FTUs)`을 분할하는 것을 목표로 합니다.  \n",
    "본 challenge의 설명에 따르면 조직 내의 FTU 크기, 모양, 수, 위치 정보를 찾는 것이 의학적으로 유효하다고 말합니다.  \n",
    "위 competition에서는 그 중 normal glomeruli을 분할하고자 합니다.\n",
    "\n",
    "\n",
    "데이터셋 구성은 다음과 같습니다.\n",
    "- 20 kidney tissue sections\n",
    "    - 11 fresh frozen (FF) carboxymethylcellulose(CMC) embedded\n",
    "    - 9 formalin fixed paraffin embedded (FFPE)\n",
    "- Each sample has the following data:\n",
    "    - PAS stain microscopy image(RGB-channel TIFF)\n",
    "        - The histology stained image is saved as a 24 bit RGB .tif file.\n",
    "        - The spatial resolution is .5 micron per pixel\n",
    "    - Anatomical structure segmentation mask(JSON)\n",
    "    - Glomeruli segmentation mask(JSON)\n",
    "- Known clinical metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1-2. Dataset structure\n",
    "HuBMAP 데이터셋 구조는 다음과 같습니다.\n",
    "\n",
    "- Anatomical structure(json)\n",
    "- Glomeruli label(json)\n",
    "- Whole Slide Image(tiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 외에도 race, sex, age와 같은 clinical metadata도 포함되어 있습니다.  \n",
    "-> HuBMAP-20-dataset_information.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import PIL.Image as Image\n",
    "import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuBMAP_PATH = \"./wsi/HuBMAP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubmap_data_info = pd.read_csv(os.path.join(HuBMAP_PATH, 'HuBMAP-20-dataset_information.csv'))\n",
    "hubmap_data_info.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-2-1. Whole Slide Image(.tiff)\n",
    "본 challenge의 Kidney WSI는 tiff 파일로 구성되어 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLE_FACTOR = 16 # level4 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_sample_lv4 = tiff.imread(os.path.join(HuBMAP_PATH, \"0486052bb.tiff\")) #openslide open level4\n",
    "wsi_sample_lv4 = cv2.resize(wsi_sample_lv4,\n",
    "                            (wsi_sample_lv4.shape[1]//DOWNSAMPLE_FACTOR,wsi_sample_lv4.shape[0]//DOWNSAMPLE_FACTOR),\n",
    "                            interpolation = cv2.INTER_AREA)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wsi_sample_lv4)\n",
    "plt.title(\"0486052bb.tiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-2-2. Anatomical structure(.json)\n",
    "Kidney 조직 WSI 영상의 anatomical structure를 json 파일로 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Anatomical_structure](https://www.kaggleusercontent.com/kf/47173731/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..q-BBmw8iCLU5eus2kmodNQ.0L4ZTx8opqJ45okmbMud-ycfHAyPeW6HCa6ZtukTlNPjyZpfXHIo8nLq8kE_3xrxCVsTxrd9ap0KOjmKq4CeNroqY9kk4_gRPSoWzFKJ_rehW_2IDHBTpXRgxvEUlfQyqjgTxwECcUX8X4eatGlI7B92ybYNf0rTVM45nN73zPwTyNvZKFJVmOTYKTlxRmTvOOaETigy8f29MZLtPtvqEIIz8xOPz1F-A4wmyKKht4qUOszVC6EdO-K0x7t7RxCOmWQmADFpYn-0k4FTxlRNQCDgw3o8mfmjm0MIO81oMaIYmL1QmixNbft1h613haBCL0Ee8B5EP167-O_vDySa6ZGTZONOT5vaiq6t4IyZS3ojP6AEi30l006DZ46vEKmgiIIavgLrEbGnX0iIyNdxOti2vCOZprh6Bkh4_cAQC4dh8eMyP6rqx7UvCoVWgLMNfdIApgb2xOHnWcnw5jkptSFmWhVF_0_i78W8pSNrt2iVpdCO-weFBuIB5obKv-KX2Cr-Qi-LIrbM_soi2C4aUsDpSOfR20AaxPj_LXLNhncDMoRrJgnY8ydzBh_g-FdyM51bd43KxF7ePgdeB3kIjyMnjzwDswbqqr8_DNXXvYOlgPVFTFJHKPoEUyQACQEP-QHSOyyZt4RU86xfrKrBhEW-GsRd-scuxzNXw1ySP64.v-jC0l2WA_ttSh6_KTe8XA/__results___files/__results___2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.kaggle.com/leahscherschel/dataset-details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_data_sample = json.load(open(os.path.join(HuBMAP_PATH, \"0486052bb-anatomical-structure.json\"), 'r'))\n",
    "structure_data_sample #list (x, y) 이 때, x좌표의 0은 왼쪽 y좌표의 0은 상단입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 내의 anatomical structure 개수와 구성\n",
    "len(structure_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(structure_data_sample)):\n",
    "    print(structure_data_sample[i][\"properties\"][\"classification\"][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_data_sample[1] #dict in list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json 파일은 좌표로 구성되어 있어 각 좌표를 이어주면 해당 polygon을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![coordinate_to_poly](https://qph.fs.quoracdn.net/main-qimg-2a0bdb4c01da036091c068fcc4c4ff0a.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cortex\n",
    "poly_coordi = structure_data_sample[1]['geometry']['coordinates'] #level0 coordinates \n",
    "poly_coordi = np.int32(np.array(poly_coordi) / DOWNSAMPLE_FACTOR) #level4 coordinates\n",
    "cortex_lv4 = wsi_sample_lv4.copy()\n",
    "cortex_lv4 = cv2.polylines(cortex_lv4, np.int32([poly_coordi]), True, (0, 255, 0), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(cortex_lv4)\n",
    "plt.title(\"0486052bb.tiff - Cortex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-2-3. Glomeruli label(.json, .csv)\n",
    "target인 glomeruli 또한 json 파일로 구성되어 있습니다.   \n",
    "또한 Run Length Encoding(RLE) 기법을 이용해 glomeruli mask 영상을 압축하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_annotation_sample = json.load(open(os.path.join(HuBMAP_PATH, \"0486052bb.json\"), 'r'))\n",
    "print(f\"Number of glomerulus(mask): {len(mask_annotation_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lv4 = np.zeros(wsi_sample_lv4.shape[0:2])\n",
    "cnt = 0\n",
    "for anno in mask_annotation_sample:\n",
    "    label_name = anno['properties']['classification']['name']\n",
    "    if label_name != 'glomerulus':\n",
    "        continue\n",
    "    anno_coordi = anno['geometry']['coordinates']\n",
    "    if len(anno_coordi) <= 1:\n",
    "        anno_coordi = np.int32(np.array(anno_coordi) / DOWNSAMPLE_FACTOR)\n",
    "        mask_lv4 = cv2.fillPoly(mask_lv4, anno_coordi, True)\n",
    "        cnt += 1\n",
    "    else:\n",
    "        anno_coordi = np.concatenate([np.array(anno_coordi[i]).squeeze() for i in range(len(anno_coordi))])\n",
    "        anno_coordi = np.int32(np.array(anno_coordi) / DOWNSAMPLE_FACTOR)\n",
    "        mask_lv4 = cv2.fillPoly(mask, anno_coordi, True)\n",
    "        cnt += len(anno_coordi)\n",
    "        \n",
    "print(f\"Added {cnt} glomerulus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wsi_sample_lv4)\n",
    "plt.imshow(mask_lv4, alpha=0.3)\n",
    "plt.title(\"0486052bb.tiff - glomerulus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리: WSI patch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference code: https://www.kaggle.com/mariazorkaltseva/hubmap-seresnext50-unet-dice-loss\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape[::-1]).T\n",
    "\n",
    "# New version\n",
    "def rle_encode_less_memory(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    This simplified method requires first and last pixel to be zero\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def is_tile_contains_info(img, pixel_limits, content_threshold, expected_shape):\n",
    "    \"\"\"\n",
    "    img: np.array\n",
    "    pixel_limits: tuple\n",
    "    content_threshold: float percents\n",
    "    expected_shape: tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    left_limit = np.prod(img > pixel_limits[0], axis=-1)\n",
    "    right_limit =  np.prod(img < pixel_limits[1], axis=-1)\n",
    "\n",
    "    if img.shape != expected_shape:\n",
    "        return False, 0.\n",
    "\n",
    "    percent_of_pixels = np.sum(left_limit*right_limit) / (img.shape[0] * img.shape[1])\n",
    "    return  percent_of_pixels > content_threshold, percent_of_pixels\n",
    "\n",
    "def extract_train_tiles(sample_img_path, rle_mask_sample, fname):\n",
    "    \"\"\"downsampling image and extract tiles with downsampled image\n",
    "    \"\"\"\n",
    "    print(fname)\n",
    "    sample_image = tiff.imread(sample_img_path)\n",
    "        \n",
    "    sample_mask = rle_decode(rle_mask_sample, sample_image.shape[0:2])\n",
    "    print(f\"Original Tiff image shape: {sample_image.shape}\")\n",
    "    \n",
    "    pad0 = (REDUCE_RATE*TILE_SIZE - sample_image.shape[0]%(REDUCE_RATE*TILE_SIZE))%(REDUCE_RATE*TILE_SIZE)\n",
    "    pad1 = (REDUCE_RATE*TILE_SIZE - sample_image.shape[1]%(REDUCE_RATE*TILE_SIZE))%(REDUCE_RATE*TILE_SIZE)\n",
    "    \n",
    "    sample_image = np.pad(sample_image,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                   constant_values=0)\n",
    "    sample_mask = np.pad(sample_mask,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2]],\n",
    "                  constant_values=0)\n",
    "        \n",
    "    sample_image = cv2.resize(sample_image,(sample_image.shape[1]//REDUCE_RATE,sample_image.shape[0]//REDUCE_RATE),\n",
    "                             interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    sample_mask = cv2.resize(sample_mask,(sample_mask.shape[1]//REDUCE_RATE,sample_mask.shape[0]//REDUCE_RATE),\n",
    "                             interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    print(f\"Reduced Tiff image shape: {sample_image.shape}\")\n",
    "    \n",
    "    tiles, masks = [], []\n",
    "    for x in range(0,sample_image.shape[0],TILE_SIZE):\n",
    "        for y in range(0,sample_image.shape[1],TILE_SIZE):\n",
    "            sub_image = np.float32(sample_image[x:x+TILE_SIZE,y:y+TILE_SIZE])\n",
    "            sub_mask = sample_mask[x:x+TILE_SIZE,y:y+TILE_SIZE]\n",
    "            if is_tile_contains_info(sub_image, (50, 220), 0.7, (TILE_SIZE,TILE_SIZE, 3))[0]:\n",
    "                tiles.append(sub_image)\n",
    "                masks.append(sub_mask)\n",
    "            else:\n",
    "                continue\n",
    "    if not os.path.exists(TRAIN_SAVE_DIR):\n",
    "        os.mkdir(TRAIN_SAVE_DIR)\n",
    "    if not os.path.exists(os.path.join(TRAIN_SAVE_DIR, \"wsi\")):\n",
    "        os.mkdir(os.path.join(TRAIN_SAVE_DIR, \"wsi\"))\n",
    "    if not os.path.exists(os.path.join(TRAIN_SAVE_DIR, \"mask\")):\n",
    "        os.mkdir(os.path.join(TRAIN_SAVE_DIR, \"mask\"))\n",
    "\n",
    "    count = 0\n",
    "    for tile,mask in zip(tiles,masks):\n",
    "        cv2.imwrite(os.path.join(TRAIN_SAVE_DIR, \"wsi\", f\"{fname}_{count:03}.png\"), tile)\n",
    "        cv2.imwrite(os.path.join(TRAIN_SAVE_DIR, \"mask\", f\"{fname}_{count:03}.png\"), mask)\n",
    "        count += 1\n",
    "            \n",
    "    print(f\"Length tiles\", len(tiles))\n",
    "\n",
    "def extract_test_tiles(sample_img_path, rle_mask_sample, fname):\n",
    "    \"\"\"padding + downsampling image and extract tiles with downsampled image\n",
    "    \"\"\"\n",
    "    print(fname)\n",
    "    sample_image = tiff.imread(sample_img_path)\n",
    "    \n",
    "    sample_mask = rle_decode(rle_mask_sample, sample_image.shape[0:2])\n",
    "    print(f\"Original Tiff image shape: {sample_image.shape}\")\n",
    "    \n",
    "    pad0 = (REDUCE_RATE*TILE_SIZE - sample_image.shape[0]%(REDUCE_RATE*TILE_SIZE))%(REDUCE_RATE*TILE_SIZE)\n",
    "    pad1 = (REDUCE_RATE*TILE_SIZE - sample_image.shape[1]%(REDUCE_RATE*TILE_SIZE))%(REDUCE_RATE*TILE_SIZE)\n",
    "    \n",
    "    sample_image = np.pad(sample_image,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                   constant_values=0)\n",
    "    sample_mask = np.pad(sample_mask,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2]],\n",
    "                  constant_values=0)\n",
    "        \n",
    "    sample_image = cv2.resize(sample_image,(sample_image.shape[1]//REDUCE_RATE,sample_image.shape[0]//REDUCE_RATE),\n",
    "                             interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    sample_mask = cv2.resize(sample_mask,(sample_mask.shape[1]//REDUCE_RATE,sample_mask.shape[0]//REDUCE_RATE),\n",
    "                             interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    print(f\"Reduced Tiff image shape: {sample_image.shape}\")\n",
    "    \n",
    "    tiles, masks = [], []\n",
    "    for x in range(0,sample_image.shape[0],TILE_SIZE):\n",
    "        for y in range(0,sample_image.shape[1],TILE_SIZE):\n",
    "            sub_image = np.float32(sample_image[x:x+TILE_SIZE,y:y+TILE_SIZE])\n",
    "            sub_mask = sample_mask[x:x+TILE_SIZE,y:y+TILE_SIZE]\n",
    "            tiles.append(sub_image)\n",
    "            masks.append(sub_mask)\n",
    "    if not os.path.exists(TEST_SAVE_DIR):\n",
    "        os.mkdir(TEST_SAVE_DIR)\n",
    "    if not os.path.exists(os.path.join(TEST_SAVE_DIR, \"wsi\")):\n",
    "        os.mkdir(os.path.join(TEST_SAVE_DIR, \"wsi\"))\n",
    "    if not os.path.exists(os.path.join(TEST_SAVE_DIR, \"mask\")):\n",
    "        os.mkdir(os.path.join(TEST_SAVE_DIR, \"mask\"))\n",
    "\n",
    "    count = 0\n",
    "    for tile,mask in zip(tiles,masks):\n",
    "        cv2.imwrite(os.path.join(TEST_SAVE_DIR, \"wsi\", f\"{fname}_{count:03}.png\"), tile)\n",
    "        cv2.imwrite(os.path.join(TEST_SAVE_DIR, \"mask\", f\"{fname}_{count:03}.png\"), mask)\n",
    "        count += 1\n",
    "            \n",
    "    print(f\"Length tiles\", len(tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAVE_DIR = \"./wsi/train/\"\n",
    "TEST_SAVE_DIR = \"./wsi/test/\"\n",
    "TILE_SIZE = 256\n",
    "REDUCE_RATE = 4\n",
    "hubmap_rle_info = pd.read_csv(os.path.join(HuBMAP_PATH, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = os.path.join(HuBMAP_PATH, \"0486052bb.tiff\")\n",
    "train_rle_str = hubmap_rle_info[hubmap_rle_info[\"id\"]==\"0486052bb\"].encoding.values[0]\n",
    "extract_train_tiles(train_img_path, train_rle_str, \"0486052bb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = os.path.join(HuBMAP_PATH, \"8242609fa.tiff\")\n",
    "train_rle_str = hubmap_rle_info[hubmap_rle_info[\"id\"]==\"8242609fa\"].encoding.values[0]\n",
    "extract_train_tiles(train_img_path, train_rle_str, \"8242609fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = os.path.join(HuBMAP_PATH, \"aaa6a05cc.tiff\")\n",
    "test_rle_str = hubmap_rle_info[hubmap_rle_info[\"id\"]==\"aaa6a05cc\"].encoding.values[0]\n",
    "extract_test_tiles(test_img_path, test_rle_str, \"aaa6a05cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuBMAPDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            paths, \n",
    "            mode,\n",
    "            augmentation=None,\n",
    "            preprocessing=None,\n",
    "    ):\n",
    "\n",
    "        self.paths = paths\n",
    "        self.mode = mode\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __getitem__(self, i):       \n",
    "        if self.mode in ['train', 'val']:\n",
    "            image = np.array(Image.open(self.paths[i][0]))\n",
    "            mask = np.array(Image.open(self.paths[i][1]).convert('L'))\n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "        else:\n",
    "            image = np.array(Image.open(self.paths[i]))\n",
    "\n",
    "        if self.augmentation:\n",
    "            if self.mode in ['train', 'val']:\n",
    "                sample = self.augmentation(image=image, mask=mask)\n",
    "                image, mask = sample['image'], sample['mask']\n",
    "            else:\n",
    "                sample = self.augmentation(image=image)\n",
    "                image = sample['image']\n",
    "\n",
    "        if self.preprocessing:\n",
    "            if self.mode in ['train', 'val']:\n",
    "                sample = self.preprocessing(image=image, mask=mask)\n",
    "                image, mask = sample['image'], sample['mask']\n",
    "            else:\n",
    "                sample = self.preprocessing(image=image)\n",
    "                image = sample['image']\n",
    "\n",
    "        if self.mode in ['train', 'val']:\n",
    "            return image, mask\n",
    "        \n",
    "        return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    transform_list = []\n",
    "    transform_list.append(A.RandomRotate90(p=.5))\n",
    "    transform_list.append(A.HorizontalFlip(p=.5))\n",
    "    transform_list.append(A.VerticalFlip(p=.5))\n",
    "    transform_list.append(A.Transpose(p=.5))\n",
    "    transform_list.append(A.ShiftScaleRotate(scale_limit=0.2, rotate_limit=0, shift_limit=0.2, border_mode=0, p=.5))\n",
    "    transform_list.append(\n",
    "        A.OneOf([\n",
    "            A.RandomBrightness(limit=.2, p=1), \n",
    "            A.RandomContrast(limit=.2, p=1), \n",
    "            A.RandomGamma(p=1)\n",
    "        ], p=.5)\n",
    "    )\n",
    "    transform_list.append(\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3, p=1),\n",
    "            A.MedianBlur(blur_limit=3, p=1)\n",
    "        ], p=.1)\n",
    "    )\n",
    "    transform_list.append(\n",
    "        A.OneOf([\n",
    "            A.RandomContrast(p=1),\n",
    "            A.HueSaturationValue(p=1)\n",
    "        ], p=.9)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return A.Compose(transform_list)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "        A.Normalize(mean=(0.65459856,0.48386562,0.69428385), \n",
    "                       std=(0.15167958,0.23584107,0.13146145), \n",
    "                       max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "        A.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path_ls = glob.glob('./wsi/train/wsi/*')\n",
    "train_img_path_ls = [file for file in train_img_path_ls if file.endswith(\".png\")]\n",
    "train_img_path_ls.sort()\n",
    "train_mask_path_ls = glob.glob('./wsi/train/mask/*')\n",
    "train_mask_path_ls = [file for file in train_mask_path_ls if file.endswith(\".png\")]\n",
    "train_mask_path_ls.sort()\n",
    "train_path_ls = list(zip(train_img_path_ls, train_mask_path_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path_ls = glob.glob('./wsi/test/wsi/*')\n",
    "test_img_path_ls = [file for file in test_img_path_ls if file.endswith(\".png\")]\n",
    "test_img_path_ls.sort()\n",
    "test_mask_path_ls = glob.glob('./wsi/test/mask/*')\n",
    "test_mask_path_ls = [file for file in test_mask_path_ls if file.endswith(\".png\")]\n",
    "test_mask_path_ls.sort()\n",
    "test_paths = list(zip(test_img_path_ls, test_mask_path_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, val_paths = train_test_split(train_path_ls,\n",
    "                                        test_size=0.2,  \n",
    "                                        random_state=0,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dataset = HuBMAPDataset(train_paths,\n",
    "                              'train',\n",
    "                              augmentation=get_training_augmentation(),\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check augmentation \n",
    "for i in range(10):\n",
    "    image, mask = aug_dataset[3]\n",
    "    visualize(image=image, mask=mask.squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 모델 학습\n",
    "segmentation 모델을 모아놓은 library를 활용합니다.  \n",
    "segmentation models by. qubvel  \n",
    "https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet\n",
    "실습에서 사용할 모델은 Unet입니다.  \n",
    "Unet은 말 그대로 U자 모양으로 생긴 모델로   \n",
    "입력 영상의 context 포착을 위한 contracting path와    \n",
    "추출한 feature map을 upsampling하는 구간인 expanding path로 구성되어 있습니다.  \n",
    "최종 output은 각 pixel에 target 유무에 대한 probability로 나오게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Unet](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvLAIi%2FbtqBCmVoUFu%2FsaODCYPMvM5Siesq8s3gP1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice loss\n",
    "$\n",
    "DL = 1 - {{2p_{i1}y_{i1} +\\gamma}\\over{p_{i1}^{2}y_{i1}^{2} +\\gamma}}\n",
    "$\n",
    "실습에서 사용하는 loss function은 dice loss로 dice coefficient를 loss로 활용합니다.  \n",
    "dice coefficient는 prediction과 ground truth의 overlap area에 2를 곱하고  \n",
    "prediction과 ground truth 영역을 합한 값으로 나누어 구합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dice loss](https://i.stack.imgur.com/OsH4y.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HuBMAPDataset(train_paths, \n",
    "                              'train',\n",
    "                              augmentation=get_training_augmentation(), \n",
    "                              preprocessing=get_preprocessing()\n",
    "                            )\n",
    "valid_dataset = HuBMAPDataset(val_paths,\n",
    "                              'val',\n",
    "                              augmentation=get_validation_augmentation(), \n",
    "                              preprocessing=get_preprocessing()\n",
    "                            )\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                            batch_size=16, \n",
    "                                            shuffle=True, \n",
    "                                            num_workers=4, \n",
    "                                            pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                            batch_size=8, \n",
    "                                            shuffle=False, \n",
    "                                            num_workers=4, \n",
    "                                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'se_resnext50_32x4d'\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=None, \n",
    "    in_channels=3,\n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./ckpt/Unet_Resnext50\"\n",
    "\n",
    "try: \n",
    "    if not os.path.exists(save_path): \n",
    "        os.makedirs(save_path)\n",
    "        print(f\"New directory!: {save_path}\")\n",
    "except OSError: \n",
    "    print(\"Error: Failed to create the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 10 epochs\n",
    "epoch = 10\n",
    "max_score = 0\n",
    "\n",
    "for i in range(0, epoch-1):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, os.path.join(save_path, 'best_model01.pth'))\n",
    "        print('New Record!')\n",
    "    \n",
    "    torch.save(model, os.path.join(save_path, 'final_model01.pth'))\n",
    "        \n",
    "    if i == 100:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-1. Patch-wise validation\n",
    "평가는 patch-wise와 whole slide-wise 두가지로 진행합니다.  \n",
    "먼저 patch-wise로 predict mask의 dice coefficient 평균을 구하고  \n",
    "실제로 어떤 형태로 prediction되는지 시각화해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path_ls = glob.glob('./wsi/test/wsi/*')\n",
    "test_img_path_ls = [file for file in test_img_path_ls if file.endswith(\".png\")]\n",
    "test_img_path_ls.sort()\n",
    "test_mask_path_ls = glob.glob('./wsi/test/mask/*')\n",
    "test_mask_path_ls = [file for file in test_mask_path_ls if file.endswith(\".png\")]\n",
    "test_mask_path_ls.sort()\n",
    "test_paths = list(zip(test_img_path_ls, test_mask_path_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(os.path.join(\"./ckpt/Unet_Resnext50\", 'best_model01.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code reference: https://gist.github.com/gergf/acd8e3fd23347cb9e6dc572f00c63d79\n",
    "def dice(true_mask, pred_mask, non_seg_score=1.0):\n",
    "    \"\"\"\n",
    "        Computes the Dice coefficient.\n",
    "        Args:\n",
    "            true_mask : Array of arbitrary shape.\n",
    "            pred_mask : Array with the same shape than true_mask.  \n",
    "        \n",
    "        Returns:\n",
    "            A scalar representing the Dice coefficient between the two segmentations. \n",
    "        \n",
    "    \"\"\"\n",
    "    assert true_mask.shape == pred_mask.shape\n",
    "\n",
    "    true_mask = np.asarray(true_mask).astype(np.bool_)\n",
    "    pred_mask = np.asarray(pred_mask).astype(np.bool_)\n",
    "\n",
    "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
    "    im_sum = true_mask.sum() + pred_mask.sum()\n",
    "    if im_sum == 0:\n",
    "        return non_seg_score\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(true_mask, pred_mask)\n",
    "    return 2. * intersection.sum() / im_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HuBMAPDataset(test_img_path_ls, \n",
    "                             'test',\n",
    "                             preprocessing=get_preprocessing()\n",
    "                        )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=1, \n",
    "                                              shuffle=False, \n",
    "                                              num_workers=4)\n",
    "\n",
    "test_dataset_vis = HuBMAPDataset(test_paths, 'val')\n",
    "\n",
    "tot_patch_dice_coef = 0\n",
    "best_model.eval()\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    \n",
    "    image_vis, gt = test_dataset_vis[i] #.astype('uint8')\n",
    "    image = test_dataset[i]\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pr_mask = best_model.predict(x_tensor)\n",
    "        pr_mask = (pr_mask.squeeze().cpu().numpy().round().astype('uint8'))\n",
    "    tot_patch_dice_coef += dice(gt.squeeze(), pr_mask)\n",
    "print(f\"Average of patch-wise dice coefficient: {tot_patch_dice_coef/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30,50):\n",
    "    \n",
    "    image_vis, gt = test_dataset_vis[i] #.astype('uint8')\n",
    "    image = test_dataset[i]\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pr_mask = best_model.predict(x_tensor)\n",
    "        pr_mask = (pr_mask.squeeze().cpu().numpy().round().astype('uint8'))\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth = gt,\n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습의 편의를 위해 미리 학습한 weight를 갖고 평가를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(os.path.join(\"./wsi/best_ckpt\", 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HuBMAPDataset(test_img_path_ls, \n",
    "                             'test',\n",
    "                             preprocessing=get_preprocessing()\n",
    "                        )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=1, \n",
    "                                              shuffle=False, \n",
    "                                              num_workers=4)\n",
    "\n",
    "test_dataset_vis = HuBMAPDataset(test_paths, 'val')\n",
    "\n",
    "tot_patch_dice_coef = 0\n",
    "best_model.eval()\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    \n",
    "    image_vis, gt = test_dataset_vis[i] #.astype('uint8')\n",
    "    image = test_dataset[i]\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pr_mask = best_model.predict(x_tensor)\n",
    "        pr_mask = (pr_mask.squeeze().cpu().numpy().round().astype('uint8'))\n",
    "    tot_patch_dice_coef += dice(gt.squeeze(), pr_mask)\n",
    "print(f\"Average of patch-wise dice coefficient: {tot_patch_dice_coef/len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30,50):\n",
    "    \n",
    "    image_vis, gt = test_dataset_vis[i] #.astype('uint8')\n",
    "    image = test_dataset[i]\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pr_mask = best_model.predict(x_tensor)\n",
    "        pr_mask = (pr_mask.squeeze().cpu().numpy().round().astype('uint8'))\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth = gt,\n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-2. Whole image-wise validation\n",
    "patch로 나누어 결과를 출력하고, 다시 Whole Slide Image로 합치는 과정이 필요합니다.  \n",
    "합친 후, whole image에서의 평가를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slide_tiles(file_paths, ID):\n",
    "    def check_id(file_path):\n",
    "        return os.path.splitext(os.path.basename(file_path))[0].split('_')[0]\n",
    "    tile_ls = [path for path in file_paths if check_id(path)==ID]\n",
    "    return sorted(tile_ls)\n",
    "\n",
    "IDX = 'aaa6a05cc'\n",
    "slide_paths = extract_slide_tiles(test_img_path_ls, IDX)\n",
    "\n",
    "# padded and reduced shape of corresponding image\n",
    "height, width = (4864, 3328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one slide predictions\n",
    "test_dataset = HuBMAPDataset(slide_paths, \n",
    "                             'test',\n",
    "                             preprocessing=get_preprocessing()\n",
    "                        )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset_vis = HuBMAPDataset(slide_paths, 'test')\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "mask_preds = []\n",
    "for i, image in enumerate(tqdm(test_dataloader)):\n",
    "    image_vis = test_dataset_vis[i].astype('uint8')\n",
    "    \n",
    "    image = image.to(DEVICE)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        mask_pred = best_model(image)\n",
    "        mask_pred = mask_pred.squeeze().cpu().numpy().round().astype('uint8')\n",
    "    mask_preds.append(np.expand_dims(mask_pred, axis=0))\n",
    "    \n",
    "\n",
    "mask_preds = np.concatenate(mask_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch-wise image를 합치는 과정\n",
    "merge_image = np.zeros((height, width, 3))\n",
    "\n",
    "k = 0\n",
    "for i in range(0, height // TILE_SIZE):\n",
    "    for j in range(0, width // TILE_SIZE):\n",
    "        image = np.array(Image.open(slide_paths[k]))\n",
    "        merge_image[i*TILE_SIZE:i*TILE_SIZE + TILE_SIZE, j*TILE_SIZE:j*TILE_SIZE + TILE_SIZE, :] = image\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction mask를 합치는 과정\n",
    "merge_mask = np.zeros((height, width))\n",
    "\n",
    "k = 0\n",
    "for i in range(0, height // TILE_SIZE):\n",
    "    for j in range(0, width // TILE_SIZE):\n",
    "        merge_mask[i*TILE_SIZE:i*TILE_SIZE + TILE_SIZE, j*TILE_SIZE:j*TILE_SIZE + TILE_SIZE] = mask_preds[k]\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth mask 불러오기\n",
    "sample_img_path = os.path.join(HuBMAP_PATH, \"aaa6a05cc.tiff\") \n",
    "rle_mask_sample = hubmap_rle_info[hubmap_rle_info[\"id\"]==IDX].encoding.values[0]\n",
    "sample_image = tiff.imread(sample_img_path)\n",
    "gt_mask = rle_decode(rle_mask_sample, sample_image.shape[0:2])\n",
    "pad0 = (REDUCE_RATE*TILE_SIZE - sample_image.shape[0]%(REDUCE_RATE*TILE_SIZE))%(REDUCE_RATE*TILE_SIZE)\n",
    "pad1 = (REDUCE_RATE*TILE_SIZE - sample_image.shape[1]%(REDUCE_RATE*TILE_SIZE))%(REDUCE_RATE*TILE_SIZE)\n",
    "gt_mask = np.pad(gt_mask,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2]],\n",
    "              constant_values=0)\n",
    "gt_mask = cv2.resize(gt_mask,(gt_mask.shape[1]//REDUCE_RATE,gt_mask.shape[0]//REDUCE_RATE),\n",
    "                         interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Whole image-wise dice coefficient: {dice(gt_mask, merge_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,15))\n",
    "ax[0].imshow(merge_image.astype('uint8'))\n",
    "ax[0].imshow(gt_mask, cmap='coolwarm', alpha=0.5)\n",
    "ax[0].set_title(\"ground truth\")\n",
    "ax[1].imshow(merge_image.astype('uint8'))\n",
    "ax[1].imshow(merge_mask.astype('uint8'), cmap='coolwarm', alpha=0.5)\n",
    "ax[1].set_title(\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-3. IoU score\n",
    "IoU score는 segmentation model을 평가하는 지표 중 하나로,  \n",
    "예측 결과와 Ground Truth 간 교집합 영역 넓이/ 합집합 영역 넓이로 점수를 매깁니다.  \n",
    "일치할 경우 1, 어느 하나 매치하지 않다면 score는 0이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iou](https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pyimagesearch.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(y_pred, y_true):\n",
    "     # ytrue, ypred is a flatten vector\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_true = y_true.flatten()\n",
    "    current = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    # compute mean iou\n",
    "    intersection = np.diag(current)\n",
    "    ground_truth_set = current.sum(axis=1)\n",
    "    predicted_set = current.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / union.astype(np.float32)\n",
    "    return np.mean(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_iou(merge_mask, gt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
